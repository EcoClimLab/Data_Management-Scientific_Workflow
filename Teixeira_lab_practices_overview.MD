# Scientific workflow and data management procedures
## ForestGEO Ecosystems & Climate Lab @ SCBI

The following are guidelines for scientic workflow and data management in Dr. Anderson-Teixeira's lab. These guidelines are informed by best-practices guidance for scientific workflow and data management and adjusted to fit the needs of our lab. They are not intended as strict rules (especially when it comes to details), but are designed to ensure good practices and consistency throughout the lab. 

Researchers in the lab are expected to be familiar with lab procedures (outlined here) and ensure that their practices are compatible with these procedures. Pre-PhD researchers are asked to follow these guidelines closely, and should view implementation of these practices as a learning opportunity that will benefit their future scientific careers. Postdocs may have more leeway to follow their own system, providing that they use solid scientific workflow/ data management practices that can be integrated with lab practices. Researchers in the lab should develop plans for data management and scientific workflow in conjunction with their supervisor at the beginning of each project. 

This is a working document, and lab procedures are subject to change. Lab members are encouraged to provide feedback and ideas to help improve practices. 

## Overview
- Scientific workflow is grouped into “projects”. A project is typically creation of a data set and/or an intended publication.
- Each project has shared materials managed in two locations:
  - GitHub: data, code, and results. Access to the Github repository will be given to lab members working on data production and analysis. GitHub is also used to manage scientific workflow/ track "issues". 
  - Dropbox: manuscript, references, other supporting documents. Access to the Dropbox repository will be given to all project collaborators.
- Data work is separated into two distinct (but often closely integrated) phases:
  - [Data creation](https://github.com/EcoClimLab/Data_Management-Scientific_Workflow/tree/master)- This phase consists of collecting, entering, and checking data. Data creators are responsible to correct data errors/ provide updated versions.
  - [Data analysis]()- This phase consists of preparing code to analyze raw data files. Data analysts do not change the original files – everything goes in the script. Ideally, all data processing is automated, such that script can generate figures and tables for publication by running a single script. 


## Open access policy
Scientific data and code are valuable resources that we wish to preserve indefinitely, long beyond the lifespan of a project or even the researchers involved. Policies of the U.S. government, the Smithsonian, funding agencies, and most journals require that scientific data be made publicly available in a timely manner. Moreover, ensuring preservation of work effort and reproducibility of results requires public archiving of code. Our lab strives to follow best practices so as to ensure the long-term value of our data and code. **All researchers should work under the expectation that all data and code will be made public**, either immediately or upon publication of the first manuscript using them.

## Tools
All new members should install, create accounts (if necessary), and get oriented to the following tools, which are essential for collaborative projects in our lab: 
- Dropbox, including the desktop app. Note that Smithsonian provides free Dropbox Business accounts.
- GitHub, including the desktop app. 
- Data analysis software (R and/ or Matlab)
- A reference manager: Zotero and/ or Mendeley, or one that is intercompatible with both.
